This week's assignment dealt with processing data that comes as a real-time stream. For that we had to utilize a Spark library called Structured Streaming. 
The data I worked on was a stream of simulated Runescape transactions generated by a python script that was constantly running on my docker container.

After starting the stream and importing the necessary libraries, the first step was to define a streaming Dataframe and provide it the port from which the data would come, in my case port 9999.
Then, a second Dataframe was defined to take a sample from the stream and store it in memory. 

After defining them, a Spark job on these dataframes was executed - it streamed the data for 1 second and saved it in memory. 
Here I encountered my first problem -  for some reason the streaming data did not arrive to the memory. 
That happened because the 1 second time window was not enough to set everything up and receive the stream.
When I changed the time window to 5 seconds, it worked properly and provided me with data about 462 Runescape transactions. I could inspect it with a basic SQL query: ![brave_V3JCpPD9Fc](https://user-images.githubusercontent.com/49609518/117133243-4d6a8380-ada4-11eb-83ad-50bb3e01d828.png)

Then, I needed to process the transaction descriptions into a more structured representations. 

I extracted the elements of each transaction: material, weapon and price, using a regular expression and an SQL query.
![brave_5CK6XUidjM](https://user-images.githubusercontent.com/49609518/117134434-df26c080-ada5-11eb-801e-8b64f4c3650b.png)

This worked well, but it was still not proper stream processing - it first took a sample, saved it and then processed it as a static dataset.

To make it process in real time, I just had to connect the query directly to the streaming DF, instead of memory DF. Then, when the query was defined, I just activated the stream
and let it run for a few seconds. Instead of collecting the sentences and saving them, it processed them on the go and only saved the extracted data: material, weapon and price.
The processed data got saved on the disk, and from there it could be analyzed further.

To play with SQL a bit, I analyzed which types of weapons are bought most often and what the average price is. It turns out that all the weapon types are almost equally as popular,
but their price varies a lot, with dagggers being the cheapest and halberd the most expensive. It makes sense, considering the size of the weapons and the amount of material necessary to make them.
![brave_4aik0Dn4nk](https://user-images.githubusercontent.com/49609518/117139541-a5a58380-adac-11eb-82bd-cbf1a3a1a12b.png)
![brave_NMnenKjZQI](https://user-images.githubusercontent.com/49609518/117139548-a807dd80-adac-11eb-94b2-4a5e187a46bb.png)

After going through the assignment, I learned how to connect a cluster to a data stream coming from outside, how to process the data on the go and save the results in memory.
These are definitely essential skills for many real-world big data applications, with large amounts of data coming in every second that need to be processed but cannot be stored.
